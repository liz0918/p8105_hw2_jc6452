Homework 2
================

# Problem 1

## Loading data and cleaning up

``` r
nyc_transit = read_csv('data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv', na = "") %>% 
  janitor::clean_names() %>% 
      pivot_longer(
      cols = route1:route11,
      names_to = "route_number",
      values_to = "route_name",
      values_transform = list(route_name = as.character),
      names_prefix = "route"
    ) %>% 
  mutate(
    entry = case_match(
    entry,
    "YES" ~ TRUE,
    "NO" ~ FALSE
  )) %>% 
  select("line","station_name","station_latitude","station_longitude","route_number","route_name", "entry", "vending", "entrance_type", "ada") 
```

    ## Rows: 1868 Columns: 32
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (22): Division, Line, Station Name, Route1, Route2, Route3, Route4, Rout...
    ## dbl  (8): Station Latitude, Station Longitude, Route8, Route9, Route10, Rout...
    ## lgl  (2): ADA, Free Crossover
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

The above data set describes the nyc transit line, including details
regarding, the line, station name, station’s latitude/longitude, its
route number and name, presence of entry and vending, type of entrance
and its ADA compliance. As the data set came a bit messy, I have
organized the `route number` and `route name` accordingly. I also
changed the entry variable to `boolean` vector instead of a `character`
vector for easier calculation. Finally, I organized the columns using
`select` function in order to make the dataset more viewer-friendly and
more logical.

There are 465 distinct stations both by name and by line. There are 5148
stations that are ADA compliant. The proportion of station
entrances/exits without vending that allow entrance is 0.0369379.

# Problem 2

## Importing and cleaning dataset

``` r
mr = read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", skip = 1) %>% janitor::clean_names() %>% 
  select(dumpster:homes_powered) %>% 
  filter(dumpster != "NA") %>% 
  mutate(sports_balls = as.integer(sports_balls),
         type = "mr_trash",
         year = as.numeric(year)
  )
```

    ## New names:
    ## • `` -> `...15`
    ## • `` -> `...16`

``` r
professor = read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", skip = 1) %>% 
  janitor::clean_names() %>%
  filter(dumpster != "NA") %>% 
  mutate(type = "professor_trash")

gwynnda = professor_trash_wheel = read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", skip = 1) %>% janitor::clean_names() %>%
  filter(dumpster != "NA") %>% 
  mutate(type = "gwynnda")
```

## Merging dataset

``` r
trash_wheel =
  bind_rows(
    mr, professor, gwynnda) %>% 
  janitor::clean_names() %>% 
  select(dumpster:homes_powered)

summary(trash_wheel)
```

    ##     dumpster        month                year     
    ##  Min.   :  1.0   Length:1033        Min.   :2014  
    ##  1st Qu.: 86.0   Class :character   1st Qu.:2018  
    ##  Median :199.0   Mode  :character   Median :2020  
    ##  Mean   :245.7                      Mean   :2020  
    ##  3rd Qu.:393.0                      3rd Qu.:2022  
    ##  Max.   :651.0                      Max.   :2024  
    ##                                     NA's   :1     
    ##       date                        weight_tons    volume_cubic_yards
    ##  Min.   :2014-05-16 00:00:00.0   Min.   :0.610   Min.   : 5.00     
    ##  1st Qu.:2018-04-16 00:00:00.0   1st Qu.:2.540   1st Qu.:15.00     
    ##  Median :2020-12-26 00:00:00.0   Median :3.080   Median :15.00     
    ##  Mean   :2020-05-11 15:06:58.5   Mean   :3.038   Mean   :15.08     
    ##  3rd Qu.:2022-11-04 18:00:00.0   3rd Qu.:3.553   3rd Qu.:15.00     
    ##  Max.   :2024-06-11 00:00:00.0   Max.   :5.620   Max.   :20.00     
    ##  NA's   :1                       NA's   :1       NA's   :1         
    ##  plastic_bottles  polystyrene    cigarette_butts  glass_bottles   
    ##  Min.   :   0    Min.   :    0   Min.   :     0   Min.   :  0.00  
    ##  1st Qu.: 980    1st Qu.:  230   1st Qu.:  2800   1st Qu.: 10.00  
    ##  Median :1900    Median :  640   Median :  4800   Median : 18.00  
    ##  Mean   :2201    Mean   : 1383   Mean   : 13296   Mean   : 20.92  
    ##  3rd Qu.:2900    3rd Qu.: 2045   3rd Qu.: 12000   3rd Qu.: 28.00  
    ##  Max.   :9830    Max.   :11528   Max.   :310000   Max.   :110.00  
    ##  NA's   :2       NA's   :2       NA's   :2        NA's   :265     
    ##   plastic_bags      wrappers      sports_balls   homes_powered  
    ##  Min.   :    0   Min.   :    0   Min.   : 0.00   Min.   : 0.00  
    ##  1st Qu.:  220   1st Qu.:  900   1st Qu.: 6.00   1st Qu.:39.00  
    ##  Median :  470   Median : 1440   Median :12.00   Median :49.83  
    ##  Mean   :  927   Mean   : 2246   Mean   :13.96   Mean   :46.54  
    ##  3rd Qu.: 1115   3rd Qu.: 2580   3rd Qu.:20.00   3rd Qu.:58.08  
    ##  Max.   :13450   Max.   :20100   Max.   :56.00   Max.   :93.67  
    ##  NA's   :2       NA's   :119     NA's   :382     NA's   :70

The combined data set, `trash_wheel`, demonstrates some statistics
regarding the trash accumulated by different types of trash wheel:
Mr.Trash, Professor Trash and Gwynnda. Each type of trash wheel
collected various trash, including sports balls, wrappers, plastic
bottles, cigarette butts, polystyrene, etc. It also includes the date,
weight in ton and volume in cubic yards. The special part of the trash
wheel is that it can power the houses from the collected trash per ton,
as shown in the `homes_powered` column.

The total number of observations in the resulting data set is 1033 and
it has the following variables shown for each type of trash wheel
dumpster, month, year, date, weight_tons, volume_cubic_yards,
plastic_bottles, polystyrene, cigarette_butts, glass_bottles,
plastic_bags, wrappers, sports_balls, homes_powered. The total number of
weight of trash collected by Professor Trash Wheel is 246.74 tons.
Gwynnda collected total of 1.812^{4} cigarette butts in June of 2022.

# Problem 3

## Import and cleaning `bakers`, `bakes` and `results` data sets.

``` r
bakers = read_csv("data/gbb_datasets/bakers.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    first_name = sub("(\\w+\\.?).*", "\\1", baker_name)
  )
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
bakes = read_csv("data/gbb_datasets/bakes.csv", na = c("N/A" , "UNKNOWN", "", "Unknown")) %>%
  janitor::clean_names() %>% 
  mutate(
    baker = str_replace_all(baker,'"Jo"', "Jo")
  )
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
results = read_csv("data/gbb_datasets/results.csv", na = "NA", skip = 2) %>% 
  janitor::clean_names() %>% 
  mutate(
    baker = str_replace_all(baker,"Joanne", "Jo")
  )
```

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

The above data sets, `bakers`, `bakes`, and `results` were imported and
cleaned using the processes above. Knowing that `bakers` data set has a
column with full name of the bakers unlike `bakes` and `results` that
only have first name, I created a separate column with only the bakers’
first name. I also noticed that there was a “Jo” in `bakes` and
`results` who went by “Joanne”. Based on the season this baker was in,
it seemed like it was indicating the same person and have unified to the
name Jo. I’ve also removed any NA values from the rows accordingly for
each data set.

## Checking for completeness and correctness

``` r
missing_bakes = anti_join(bakers,bakes, by = c("first_name" = "baker", "series"))

missing_results = anti_join(bakers, results, by = c("first_name" = "baker", "series"))

missing_bakes_results = anti_join(bakers,bakes, by = c("first_name" = "baker", "series"))
```

Based on `missing_bakes` data frame, it seems like the `bakes` data
frame is missing series 9 and 10.

## Merging Dataset and exporting se

``` r
bake_off = left_join(bakes, results, by = c("baker" = "baker", "series" = "series", "episode" = "episode"))%>% 
  left_join(., bakers, by = c("baker" = "first_name", "series" = "series")) %>% relocate(baker_name, baker_age, baker_occupation, hometown, series, episode, signature_bake, show_stopper, technical, result) %>% 
  arrange(series, episode, baker_name)


write.csv(bake_off, file="data/gbb_datasets/bake_off.csv")
```

I have used `left_join` function to merge the 3 data frames together,
using appropriate column names to cobmine with for each. I have further
organized the columns into an order that is more intuitive and arranged
in the ascending order by `series`, `episode` and
`baker_name`(alphabetically). Finally, I have exported the combined
dataset into the folder that holds the original datasets.

``` r
winners =
  results %>% 
  filter(
    series <= 10, series >= 5) %>% 
  filter(
    result %in% c("WINNER", "STAR BAKER")
    ) %>% 
  select(series, episode, baker, result) %>% 
  pivot_wider(
    names_from = series,
    values_from = baker
  )

head(winners)
```

    ## # A tibble: 6 × 8
    ##   episode result     `5`     `6`    `7`       `8`    `9`     `10`    
    ##     <dbl> <chr>      <chr>   <chr>  <chr>     <chr>  <chr>   <chr>   
    ## 1       1 STAR BAKER Nancy   Marie  Jane      Steven Manon   Michelle
    ## 2       2 STAR BAKER Richard Ian    Candice   Steven Rahul   Alice   
    ## 3       3 STAR BAKER Luis    Ian    Tom       Julia  Rahul   Michael 
    ## 4       4 STAR BAKER Richard Ian    Benjamina Kate   Dan     Steph   
    ## 5       5 STAR BAKER Kate    Nadiya Candice   Sophie Kim-Joy Steph   
    ## 6       6 STAR BAKER Chetna  Mat    Tom       Liam   Briony  Steph

There were some predictable winners like Nadiya in series 6 since she
was the star baker for the last three episodes. One surprise was that
the winner for series 10 was David, a baker who was never a star baker
during the entire season.

``` r
viewers = read_csv("data/gbb_datasets/viewers.csv", na = "NA") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    cols = series_1:series_10,
    names_to = "series",
    values_to = "viewership",
    names_prefix = "series_"
  ) %>% 
  mutate(
    series = as.integer(series)
  ) %>% 
  arrange(series, episode)
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
head(viewers, 10)
```

    ## # A tibble: 10 × 3
    ##    episode series viewership
    ##      <dbl>  <int>      <dbl>
    ##  1       1      1       2.24
    ##  2       2      1       3   
    ##  3       3      1       3   
    ##  4       4      1       2.6 
    ##  5       5      1       3.03
    ##  6       6      1       2.75
    ##  7       7      1      NA   
    ##  8       8      1      NA   
    ##  9       9      1      NA   
    ## 10      10      1      NA

I imported and organized the viewership data. The first 10 rows can be
seen through the `head` function. The average viewership in season 1 was
2.77. The average viewership in season 5 was 10.0393.
